# Описание
Этот репозиторий содержит программу, детектирующую людей на видео и добавляющую результаты детекции поверх него. В качестве моделей здесь используются модели семейства YOLO от ultralytics.
# Установка
Склонируйте репозиторий через комманду `git clone https://github.com/alyohandrio/object_tracking_test_task.git`. Чтобы установить необходимые зависимости, нужно выполнить `pip install -r requiremets.txt`.
# Запуск программы
Для запуска программы используется команда `python3 main.py -c configs/{config_name}`. Примеры файлов конфигурации можно посмотреть [здесь](https://github.com/alyohandrio/object_tracking_test_task/tree/main/configs).
В файле указывается url, с которого будут загружаться веса модели, путь, где модель будет сохранена, путь до входного видео. Программа состоит из двух логических частей: детекция объектов моделью и их отрисовка.
Результаты детекции сохраняются в отдельный файл (bboxes_path в конфиге). Это сделано для того, чтобы можно было отрисовывать не все объекты а лишь часть.
Так, если убрать секцию `visualization` в конфиге, выполнится только детекция с сохранением её результатов в специальном файле. Если же убрать секцию `tracking`, то выполниться только отрисовка.
В секции `visualization` параметр `save_path` отвечает за выходной видеофайл, а необязательный параметр `ids` позволяет передать идентификаторы объектов, для которых нужна отрисовка.
Такой подход позволяет один раз провести детекцию, а затем сколько угодно раз применять отрисовку (например, для анализа конкретного объекта имеет смысл отрисовать только его).
# Семплирование
Для удобства анализа качества программы есть команда `python3 sample.py -i configs/{config_name} -o configs/{new_config_name} -n {num_samples}`.
Для видео, к которому уже был применён детектор, она случайно семплирует `num_samples` объектов и создаёт новый конфиг, в котором требуется отрисовка только этих объектов.
Входной конфиг здесь - конфиг, с которым работал детектор. Команда удалит секцию `tracking` и добавит нужные `ids` в секцию `visualization`. После этого нужно запустить основную программу с новым конфигом.
Выходной файл при этом будет `sample_{num_objects}_{save_path}` (TODO: поправить с разбиением по '/') или можно изменить его в созданном конфиге вручную.
# Анализ
Метрики, учитывающие ground truth bounding boxes здесь посчитать сложно, потому как разметка потребует много времени. Можно рассчитывать на то, что качество работы моделей семейства YOLO высоко.
Предлагается следующая метрика: отношение времени, в течение которого объект детектировался ко времени присутствия объекта в кадре.
Эта метрика предполагает, что сама точность детекции высокая и не бывает такого, что bounding box объекта перешёл с одного человека на другой, но считается всё тем же объектом.
Ручной анализ отрисовки некоторого семпла объектов показывает, что точность детекции действительно высока (хотя всё же иногда бывает, что модель определяет плакат, как человека),
а "переход" bbox с одного человека на другой редок (хотя всё же встречается).
С точки зрения метрики лучше, если модель определяет человека непрерывно от начала до конца его присутствия в кадре, чем если она определяет его несколько раз в отдельные интервалы времени.
Плюс этой метрики в простоте её подсчёта, так как в этой задаче большинство людей находятся в кадре на протяжении всего видео. Минусы - совершенно не учитывается полнота.

Из-за большого числа детекций довольно сложно честно посчитать полноту. Можно рассмотреть следующую метрику: отношение числа детекций к реальному числу людей в кадре.
Правда, для подсчёта реального числа людей надо понять кого считать человеком, так как на видео, например, присутствует множество людей на мосту, но вряд ли хочется их учитывать.
С точки зрения этой метрики чем больше число детекций, тем лучше (поэтому она на самом деле сама по себе не показывает полноту),
но наша первая метрика напротив будет штрафовать за большое число детекций одного объекта, так что разумно использовать эти метрики вместе.

Результаты:
| Модель | Среднее время в кадре  | Число предсказаний к числу людей |
| ------------- | ------------- | ------------- |
| YOLO 26n  | 0.130  | 2.43 |
| YOLO 26l  | 0.198  | 2.03 |
| YOLO 26x  | 0.187  | 2.14 |

Лучшей здесь кажется модель 26l.

Видно, что для всех моделей число детекций кратно больше реального числа объектов, то есть, модель зачастую дважды детектирует один и тот же объект.
Для решения этой проблемы предлагается следующее: нужно снизить порог уверенности принадлежности очередной точки текущему треку.
В функцию `track` модели можно передавать кастомный конфиг трекера (по умолчанию там стандартный botsort).
В текущей программе нужно будет в конфиге в `cfg["tracking"]["track_args"]["tracker"]` передать путь к новому файлу конфига.
Метрики, приведённые выше можно считать автоматически без какой-либо ручной разметки, так что можно перебрать трешхолды трекера по сетке (или иным способом) и получить значения метрик.
Затем нужно будет выбрать те значения, где число предсказаний близко к истинному числу людей, а среднее время в кадре ближе к 1 и посмотреть на возникающие там проблемы, дальнейшее решение уже будет отталкиваться от этого.

Вероятно ещё будут возникать проблемы с людьми, занимающими малую часть кадра.
Можно попробовать нарезать видео по пространственным координатам на пересекающиеся участки и искуственно (специальными программами или нейросетевыми методами) увеличить размерность изображения,
затем провести детекцию на этих "подвидео", но способ сложный, так как нужно будет соединять треки с разных "подвидео".
Можно попробовать обработку одного участка видео, где много людей малого размера, чтобы понять может ли эта идея вообще быть потенциально полезной.

Результаты для трёх моделей и семпл 30 объектов для них можно посмотреть здесь: https://disk.yandex.ru/d/uNj1UWJ2KoVf8g
